#!/usr/bin/env python3

"""54gene WGS germline pipeline

AUTHORS:
    B. Ballew

This pipeline takes demultiplexed, adapter-removed, compressed fastqs as input,
and generates QC reports, aligned BAMs, gVCFs, and multi-sample VCFs.

"""

import os
import datetime
import subprocess
import glob

# temporary variable assignments for testing:
nt = 2
tempDir = "temp/"
# https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/


bed = "resources/Homo_sapiens_assembly38.bed"  # change if not WGS - put switch in config?


# reference the config file
configfile: "config/config.yaml"


# import variables from config
sampleFile = config["sampleFile"]


# set a datetime stamp for GATK DBImport step
currentDt = datetime.datetime.now()
dt = currentDt.strftime("%Y%m%d%H%M%S")

# parse manifest file
# note that the lab says they're not planning on splitting samples
# across lanes - that makes this really easy!
# should double-check that sample_ID will always be unique
# otherwise consider appending other info like lane/well/runID
# will want to add functionality to analyze across multiple runs


def read_in_manifest(s):
    """
    """
    d = {}
    with open(s) as f:
        for line in f:
            (rg, sm, read1, read2) = line.split()
            d[rg] = sm, read1, read2
    return d


def create_samples_set(d):
    """
    """
    sm_set = set()
    for (key, value) in d.items():
        sm_set.add(value[0])
    return list(sm_set)


def get_read1_fastq(wildcards):
    """
    """
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return read1


def get_read2_fastq(wildcards):
    """
    """
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return read2


def get_sm(wildcards):
    """
    """
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return sm


def list_markdup_inputs(wildcards):
    """
    Lists full path and file name for each readgroup bam that
    has the same sample name (SM).  Used to create input string
    for Sentieon command (-i /path/to/sample.bam -i
    /path/to/sample2.bam ...) for rule score_duplicates and
    remove_duplicates.
    """
    l1 = []
    for (keys, vals) in sampleDict.items():
        if wildcards.sample in vals:
            l1.append("results/mapped/" + wildcards.sample + "/" + keys + ".bam")
    s1 = " INPUT=".join(l1)
    return s1


sampleDict = read_in_manifest(sampleFile)
SAMPLES = create_samples_set(sampleDict)

# Previously read from dict, but now that I'm programmatically retrieving
# the ref genome and index files, it's easier to just have a hard-coded list
# and trade off the flexibility of changing ref genomes
chromList = [
    "chr1",
    "chr2",
    "chr3",
    "chr4",
    "chr5",
    "chr6",
    "chr7",
    "chr8",
    "chr9",
    "chr10",
    "chr11",
    "chr12",
    "chr13",
    "chr14",
    "chr15",
    "chr16",
    "chr17",
    "chr18",
    "chr19",
    "chr20",
    "chr21",
    "chr22",
    "chrX",
    "chrY",
    "chrM",
]


def get_DBImport_path1(wildcards):
    return glob.glob(
        "results/HaplotypeCaller/DBImport/"
        + dt
        + "_"
        + wildcards.chrom
        + "/"
        + wildcards.chrom
        + "*/genomicsdb_meta_dir/genomicsdb_meta*.json"
    )


def get_DBImport_path2(wildcards):
    path = "".join(
        glob.glob("results/HaplotypeCaller/DBImport/" + dt + "_" + wildcards.chrom + "/*/__*/")
    )
    myList = []
    if os.path.exists(path):
        myList = [
            "AD.tdb",
            "AD_var.tdb",
            "ALT.tdb",
            "ALT_var.tdb",
            "BaseQRankSum.tdb",
            "__book_keeping.tdb.gz",
            "__coords.tdb",
            "DP_FORMAT.tdb",
            "DP.tdb",
            "DS.tdb",
            "END.tdb",
            "ExcessHet.tdb",
            "FILTER.tdb",
            "FILTER_var.tdb",
            "GQ.tdb",
            "GT.tdb",
            "GT_var.tdb",
            "ID.tdb",
            "ID_var.tdb",
            "InbreedingCoeff.tdb",
            "MIN_DP.tdb",
            "MLEAC.tdb",
            "MLEAC_var.tdb",
            "MLEAF.tdb",
            "MLEAF_var.tdb",
            "MQRankSum.tdb",
            "PGT.tdb",
            "PGT_var.tdb",
            "PID.tdb",
            "PID_var.tdb",
            "PL.tdb",
            "PL_var.tdb",
            "QUAL.tdb",
            "RAW_MQandDP.tdb",
            "ReadPosRankSum.tdb",
            "REF.tdb",
            "REF_var.tdb",
            "SB.tdb",
            "__tiledb_fragment.tdb",
        ]
        myList = [path + file for file in myList]
    return myList


include: "rules/filter.smk"


##### Wildcard constraints #####
wildcard_constraints:
    sample="|".join(SAMPLES),
    rg="|".join(sampleDict.keys()),


rule all:
    input:
        "results/multiqc/multiqc.html",
        "results/HaplotypeCaller/filtered/HC.variant_calling_detail_metrics",


rule get_resources:
    output:
        "resources/Homo_sapiens_assembly38.fasta",
        "resources/Homo_sapiens_assembly38.dict",
        "resources/Homo_sapiens_assembly38.fasta.64.alt",
        "resources/Homo_sapiens_assembly38.fasta.64.amb",
        "resources/Homo_sapiens_assembly38.fasta.64.ann",
        "resources/Homo_sapiens_assembly38.fasta.64.bwt",
        "resources/Homo_sapiens_assembly38.fasta.64.pac",
        "resources/Homo_sapiens_assembly38.fasta.64.sa",
        "resources/Homo_sapiens_assembly38.fasta.fai",
        "resources/Homo_sapiens_assembly38.known_indels.vcf.gz",
        "resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi",
        "resources/Homo_sapiens_assembly38.dbsnp138.vcf",
        "resources/Homo_sapiens_assembly38.dbsnp138.vcf.idx",
    conda:
        "envs/aws.yaml"
    shell:
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dict resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.alt resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.amb resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.ann resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.bwt resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.pac resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.sa resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx resources/ --no-sign-request"


rule symlink_fastqs:
    """
    """
    input:
        r1=get_read1_fastq,
        r2=get_read2_fastq,
    output:
        r1="results/input/{sample}_{rg}_r1.fastq.gz",
        r2="results/input/{sample}_{rg}_r2.fastq.gz",
    shell:
        "ln -s {input.r1} {output.r1} && "
        "ln -s {input.r2} {output.r2}"


rule fastqc:
    """
    """
    input:
        r1="results/input/{sample}_{rg}_r1.fastq.gz",
        r2="results/input/{sample}_{rg}_r2.fastq.gz",
    output:
        html1="results/fastqc/{sample}_{rg}_r1_fastqc.html",
        zip1="results/fastqc/{sample}_{rg}_r1_fastqc.zip",
        html2="results/fastqc/{sample}_{rg}_r2_fastqc.html",
        zip2="results/fastqc/{sample}_{rg}_r2_fastqc.zip",
    threads: nt
    conda:
        "envs/fastqc_multiqc.yaml"
    shell:
        "fastqc {input.r1} --quiet -t {threads} --outdir=results/fastqc/ && "
        "fastqc {input.r2} --quiet -t {threads} --outdir=results/fastqc/"


rule multiqc:
    """
    """
    input:
        expand("results/fastqc/{sample}_{rg}_r1_fastqc.zip", sample=SAMPLES, rg=sampleDict.keys()),
        expand("results/fastqc/{sample}_{rg}_r2_fastqc.zip", sample=SAMPLES, rg=sampleDict.keys()), # dedup metrics and samtools-stats can go here too when ready
    output:
        "results/multiqc/multiqc.html",
    params:
        outDir="results/multiqc/",
        outName="multiqc.html", # inDirs=expand("results/fastqc/{sample}_L001_R1_001/",sample=SAMPLES),expand("results/fastqc/{sample}_L001_R2_001/",sample=SAMPLES)
    conda:
        "envs/fastqc_multiqc.yaml"
    shell:
        "multiqc --force -o {params.outDir} -n {params.outName} {input}"


rule quality_trimming:
    """Assume paired-end.  May want to tweak params; possibly put in config.
    """
    input:
        r1=get_read1_fastq,
        r2=get_read2_fastq,
    output:
        r1_paired="results/paired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2_paired="results/paired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
        r1_unpaired="results/unpaired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2_unpaired="results/unpaired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
    params:
        lead=12,
        trail=12,
        window="4:15",
        minlen=36,
    threads: nt
    conda:
        "envs/trimmomatic.yaml"
    shell:
        "trimmomatic PE "
        "-threads {threads} "
        "-phred33 "
        "{input.r1} {input.r2} "
        "{output.r1_paired} {output.r1_unpaired} "
        "{output.r2_paired} {output.r2_unpaired} "
        "LEADING:{params.lead} "
        "TRAILING:{params.trail} "
        "SLIDINGWINDOW:{params.window} "
        "MINLEN:{params.minlen}" #ILLUMINACLIP:/path/to/adapters/TruSeq3-PE-2.fa:2:30:10


rule align_reads:
    """
    Aligns with BWA; this step combines R1 and R2 data into a single
    output bam.

    For samples where multiple fastq files represent a single sample,
    e.g. if a sample was run over several lanes, each R1/R1 fastq pair
    will be aligned separately here.  Then the aligned bams from a single
    sample will be combined during the dedup step below.

    """
    input:
        r="resources/Homo_sapiens_assembly38.fasta",
        r1="results/paired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2="results/paired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
        d="resources/Homo_sapiens_assembly38.dict",
        fai="resources/Homo_sapiens_assembly38.fasta.fai",
        amb="resources/Homo_sapiens_assembly38.fasta.64.amb",
        ann="resources/Homo_sapiens_assembly38.fasta.64.ann",
        bwt="resources/Homo_sapiens_assembly38.fasta.64.bwt",
        pac="resources/Homo_sapiens_assembly38.fasta.64.pac",
        sa="resources/Homo_sapiens_assembly38.fasta.64.sa",
    output:
        temp("results/mapped/{sample}/{rg}.bam"),
    params:
        pl="ILLUMINA",
        sort_order="coordinate",
        rg="{rg}",
        sm=get_sm,
    threads: nt
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "bwa mem "
        "-K 10000000 -M "
        '-R "@RG\\tCN:54gene\\tID:{params.rg}\\tSM:{params.sm}\\tPL:{params.pl}\\tLB:N/A" '
        "-t {threads} "
        "{input.r} {input.r1} {input.r2} | "
        "samtools sort -@ {threads} -o {output} - " # note - wanted to use bwa-mem2, but doesn't seem to be working via conda


rule mark_duplicates:
    """Mark duplicates and merge multiple bams per sample.

    If there are multiple fastq pairs per sample (e.g. if the sample
    is split over multiple lanes), I am waiting to merge them until
    this step, instead of concatenating them prior to alignment.  This
    is to help maintain some additional parallelization going in to
    alignment.
    """
    input:
        expand("results/mapped/{{sample}}/{rg}.bam", rg=sampleDict.keys()),
    output:
        bam=temp("results/dedup/{sample}.bam"),
        metrics="results/dedup/{sample}.metrics.txt",
    params:
        list_markdup_inputs,
    conda:
        "envs/picard.yaml"
    shell:
        "picard MarkDuplicates "
        "REMOVE_DUPLICATES=true "
        "INPUT={params} "
        "OUTPUT={output.bam} "
        "METRICS_FILE={output.metrics}"


rule recalibrate_bams:
    """
    """
    input:
        bam="results/dedup/{sample}.bam",
        r="resources/Homo_sapiens_assembly38.fasta",
        d="resources/Homo_sapiens_assembly38.dict",
        snps="resources/Homo_sapiens_assembly38.dbsnp138.vcf",
        s_index="resources/Homo_sapiens_assembly38.dbsnp138.vcf.idx",
        indels="resources/Homo_sapiens_assembly38.known_indels.vcf.gz",
        i_index="resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi",
    output:
        "results/bqsr/{sample}.recal_table",
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk BaseRecalibrator "
        "-R {input.r} "
        "-I {input.bam} "
        "--known-sites {input.snps} "
        "--known-sites {input.indels} "
        "-O {output}"


rule apply_bqsr:
    """
    """
    input:
        bam="results/dedup/{sample}.bam",
        r="resources/Homo_sapiens_assembly38.fasta",
        d="resources/Homo_sapiens_assembly38.dict",
        recal="results/bqsr/{sample}.recal_table",
    output:
        "results/bqsr/{sample}.bam",
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk ApplyBQSR "
        "-R {input.r} "
        "-I {input.bam} "
        "--bqsr-recal-file {input.recal} "
        "-O {output}"


rule index_bams:
    """
    """
    input:
        "results/bqsr/{sample}.bam",
    output:
        "results/bqsr/{sample}.bam.bai",
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "samtools index {input} {output}"


rule samtools_stats:
    """Generate stats for bams
    """
    input:
        "results/bqsr/{sample}.bam",
    output:
        "results/alignment_stats/{sample}.txt",
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "samtools stats {input} > {output}"


rule split_bed_file:
    """Separates bed regions by chromosome.

    For DV:
    If you're not assigning a number of shards by which to divide
    and parallelize, then the pipeline will parallelize by chrom.
    To do this, we take the bed file (e.g. exome capture region)
    and split the regions by chromosome.  Subsequent steps are run
    concurrently on each of the single-chromosome bed files.

    For GATK:
    HaplotypeCaller can't be parallelized per task (e.g. threads),
    so must be run over sub-regions if you want parallelization.
    **Do we want to use the old 4000-region bed file, or is by-chrom
    sufficient?

    Note that grep exits with 0 if a match is found, 1 if no match,
    and 2 if error.  Snakemake looks for exit codes of 0 to determine
    that a job finished successfully.  No match is an acceptable outcome
    here, so the shell command below should allow match or no match.
    """
    input:
        bed,
    output:
        temp("results/split_regions/{chrom}.bed"),
    shell:
        'grep "^{wildcards.chrom}[[:space:]]" {input} > {output}; '
        "if [ $? -le 1 ]; then exit 0; else exit 1; fi"


rule HC_call_variants:
    """Call gVCFs with GATK4
    Runs over each chrom in parallel.
    """
    input:
        r="resources/Homo_sapiens_assembly38.fasta",
        d="resources/Homo_sapiens_assembly38.dict",
        fai="resources/Homo_sapiens_assembly38.fasta.fai",
        amb="resources/Homo_sapiens_assembly38.fasta.64.amb",
        ann="resources/Homo_sapiens_assembly38.fasta.64.ann",
        bwt="resources/Homo_sapiens_assembly38.fasta.64.bwt",
        pac="resources/Homo_sapiens_assembly38.fasta.64.pac",
        sa="resources/Homo_sapiens_assembly38.fasta.64.sa",
        bed="results/split_regions/{chrom}.bed",
        bam="results/bqsr/{sample}.bam",
        bai="results/bqsr/{sample}.bam.bai",
    output:
        gvcf=temp("results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf"),
        idx=temp("results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf.idx"),
    conda:
        "envs/gatk.yaml"
    shell:
        'gatk --java-options "-Xmx4G" HaplotypeCaller '
        "-R {input.r} "
        "-I {input.bam} "
        "-ERC GVCF "
        "-L {input.bed} "
        "-O {output.gvcf} "
        "-G StandardAnnotation "
        "-G StandardHCAnnotation"


rule HC_compress_gvcfs:
    """Zip and index gVCFs
    """
    input:
        gvcf="results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf",
        idx="results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf.idx",
    output:
        temp("results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf.gz"),
        temp("results/HaplotypeCaller/called/{chrom}/{sample}.g.vcf.gz.tbi"),
    conda:
        "envs/bcftools_tabix.yaml"
    shell:
        "bgzip {input.gvcf} && "
        "tabix -p vcf {input.gvcf}.gz"


rule HC_concat_gvcfs:
    """
    Not clear whether it would be fastest to concat per-chrom gvcfs and
    then genotype, or genotype and then concat.
    """
    input:
        vcfList=expand(
            "results/HaplotypeCaller/called/{chrom}/{{sample}}.g.vcf.gz", chrom=chromList
        ),
        indexList=expand(
            "results/HaplotypeCaller/called/{chrom}/{{sample}}.g.vcf.gz.tbi", chrom=chromList
        ),
    output:
        "results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz",
    params:
        lambda wildcards, input: " -I ".join(input.vcfList),
    conda:
        "envs/gatk.yaml"
    shell:
        'gatk --java-options "-Xmx4G" GatherVcfs -I {params} -O {output}'


rule HC_index_gvcf:
    input:
        "results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz",
    output:
        "results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz.tbi",
    conda:
        "envs/bcftools_tabix.yaml"
    shell:
        "tabix -p vcf {input}"


rule HC_create_each_sample_map_file:
    """Create sample file to be read by GenomicsDBImport

    Can pass all vcfs on the command line, each with a -V before it,
    but with enough samples you will hit the character limit.  Instead,
    create a sample map file and read from it.

    sample map is formatted as follows: sample_name--tab--path_to_sample_vcf per line

    See GATK's documentation for additional details.
    """
    input:
        gvcf="results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz",
        index="results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz.tbi",
    output:
        temp("results/HaplotypeCaller/DBImport/{sample}.map"),
    conda:
        "envs/bcftools_tabix.yaml"
    shell:
        "n=$(bcftools query -l {input.gvcf});"
        'echo "${{n}}\t{input.gvcf}" > {output}'


rule HC_create_cohort_map_file:
    input:
        expand("results/HaplotypeCaller/DBImport/{sample}.map", sample=SAMPLES),
    output:
        temp("results/HaplotypeCaller/DBImport/cohort.sample_map"),
    params:
        mapDir="results/HaplotypeCaller/DBImport/",
    shell:
        "for i in {params.mapDir}*.map; do cat $i >> {output}; done"


rule HC_consolidate_gvcfs:
    """Split samples by chromosome

    The output of this step includes some files that are in subdirectories
    with unpredictable names.  I've attempted to include them in the input
    of the next rule via glob in the functions below (o5 and o6 in rule
    HC_genotypeGVCFs) but this is still being tested.

    Note that DBImport requires a new or empty directory for
    --genomicsdb-workspace-path.  Possible issue when resuming a pipeline.
    Snakemake's implicit directory management results in an error when
    DBImport finds that the workspace path already exists (even though it
    seems empty?).  Removing the directory just prior to running DBImport
    seems to solve this problem, but will be problematic on resuming the
    pipeline.  Added a datetime stamp (dt) to the dir to help address this.
    However, this datetime stamp will need to be overridden if the pipeline
    is stopped and then resumed prior to completion of rule HC_genotypeGVCFs.

    What exactly is GenomicsDB for?  GenotypeGVCFs can only take one single
    input.  GenomicsDB is a method of consolidating gvcfs across samples,
    to provide the input for genotyping.  The alternative is to use
    CombineGVCFs.  See
    https://software.broadinstitute.org/gatk/documentation/article?id=11813
    for more details.

    As documented here (https://software.broadinstitute.org/gatk/documentation/
    tooldocs/4.0.11.0/org_broadinstitute_hellbender_tools_genomicsdb_
    GenomicsDBImport.php), DBImport can take up a lot of /tmp space.  GATK
    recommends using --tmp-dir to redirect to a large temp space.  Annoyingly,
    this tool appears unable to implicitly create temp subdirectories, so you
    have to explicitly include temp dir creation in the shell section.
    Note that GenotypeGVCFs below does not suffer from this directory creation
    issue.
    """
    input:
        sampleMap="results/HaplotypeCaller/DBImport/cohort.sample_map",
        gvcfList=expand(
            "results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz", sample=SAMPLES
        ),
        indexList=expand(
            "results/HaplotypeCaller/called/{sample}_all_chroms.g.vcf.gz.tbi", sample=SAMPLES
        ),
    output:
        t=temp(directory(tempDir + "HC_DBImport/{chrom}/")),
        o1=temp("results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/vcfheader.vcf"),
        o2=temp("results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/vidmap.json"),
        o3=temp("results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/callset.json"),
        o4=temp("results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/__tiledb_workspace.tdb"),
    params:
        interval="{chrom}",
        db="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}",
    conda:
        "envs/gatk.yaml"
    shell:
        "mkdir -p {output.t} && "
        "rm -r {params.db} && "
        'gatk --java-options "-Xmx20G" GenomicsDBImport '
        "--sample-name-map {input.sampleMap} "
        "--genomicsdb-workspace-path {params.db} "
        "-L {params.interval} "
        "--tmp-dir {output.t}"


rule HC_genotype_gvcfs:
    """Joint genotyping
    """
    input:
        r="resources/Homo_sapiens_assembly38.fasta",
        bed=bed,
        o1="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/vcfheader.vcf",
        o2="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/vidmap.json",
        o3="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/callset.json",
        o4="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}/__tiledb_workspace.tdb",
        o5=get_DBImport_path1,
        o6=get_DBImport_path2,
    output:
        vcf=temp("results/HaplotypeCaller/genotyped/{chrom}.vcf.gz"),
        idx=temp("results/HaplotypeCaller/genotyped/{chrom}.vcf.gz.tbi"),
    params:
        db="results/HaplotypeCaller/DBImport/" + dt + "_{chrom}",
        t=tempDir + "HC_genotype_gvcfs/{chrom}/",
    conda:
        "envs/gatk.yaml"
    shell:
        "mkdir -p {params.t} && "
        'gatk --java-options "-Xmx4G" GenotypeGVCFs '
        "-R {input.r} "
        "-V gendb://{params.db} "
        "-O {output.vcf} "
        "--tmp-dir {params.t} "
        "-stand-call-conf 30 "
        "-G StandardAnnotation "
        "-G StandardHCAnnotation"


rule HC_concat_vcfs_bcftools:
    """
    """
    input:
        vcfList=expand("results/HaplotypeCaller/genotyped/{chrom}.vcf.gz", chrom=chromList),
        indexList=expand("results/HaplotypeCaller/genotyped/{chrom}.vcf.gz.tbi", chrom=chromList),
    output:
        projectVCF=protected("results/HaplotypeCaller/genotyped/HC_variants.vcf.gz"),
        idx=protected("results/HaplotypeCaller/genotyped/HC_variants.vcf.gz.tbi"),
    params:
        tempDir + "HC_concat_vcfs_bcftools/",
    conda:
        "envs/bcftools_tabix.yaml"
    shell:
        "mkdir -p {params} && "
        "bcftools concat -a {input.vcfList} -Ou | "
        "bcftools sort -T {params} -Oz -o {output.projectVCF} && "
        "tabix -p vcf {output.projectVCF}"
