#!/usr/bin/env python3

"""54gene WGS germline pipeline

AUTHORS:
    B. Ballew

This pipeline takes demultiplexed, adapter-removed, compressed fastqs as input,
and generates QC reports, aligned BAMs, gVCFs, and multi-sample VCFs.

"""

import os
import datetime
import subprocess
import glob


# reference the config file
configfile: "config/config.yaml"


# import variables from config
sampleFile = config["sampleFile"]
nt = config["threads"]
tempDir = config["tempDir"]

# set a datetime stamp for GATK DBImport step
currentDt = datetime.datetime.now()
dt = currentDt.strftime("%Y%m%d%H%M%S")

bed = "resources/Homo_sapiens_assembly38.bed"  # change if not WGS - put switch in config?

# parse manifest file
# note that the lab says they're not planning on splitting samples
# across lanes - that makes this really easy!
# should double-check that sample_ID will always be unique
# otherwise consider appending other info like lane/well/runID
# will want to add functionality to analyze across multiple runs


def read_in_manifest(s):
    """Parse whitespace delimited manifest file."""
    d = {}
    with open(s) as f:
        for line in f:
            (rg, sm, read1, read2) = line.split()
            d[rg] = sm, read1, read2
    return d


def create_samples_set(d):
    """Generate a list of unique sample IDs from the manifest."""
    sm_set = set()
    for (key, value) in d.items():
        sm_set.add(value[0])
    return list(sm_set)


def get_read1_fastq(wildcards):
    """Return r1 fastq for a given readgroup."""
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return read1


def get_read2_fastq(wildcards):
    """Return r2 fastq for a given readgroup."""
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return read2


def get_sm(wildcards):
    """Return sample name for a given readgroup."""
    (sm, read1, read2) = sampleDict[wildcards.rg]
    return sm


def list_markdup_inputs(wildcards):
    """List path and filename for each readgroup bam with the same sample name.
    Used to create input string for Picard MarkDuplicates (INPUT=/path/to/sample.bam
    INPUT=/path/to/sample2.bam ...).  This allows merging of bams when there are
    multiple fastq pairs per sample, resulting in multiple pre-dedup bams per sample.
    """
    l1 = []
    for (keys, vals) in sampleDict.items():
        if wildcards.sample in vals:
            l1.append("results/mapped/" + wildcards.sample + "/" + keys + ".bam")
    s1 = " INPUT=".join(l1)
    return s1


sampleDict = read_in_manifest(sampleFile)
# dict where key is readgroup, values are sample, r1 fastq, r2 fastq
SAMPLES = create_samples_set(sampleDict)
# list of unique sample names

# Previously read from dict, but now that I'm programmatically retrieving
# the ref genome and index files, it's easier to just have a hard-coded list
# and trade off the flexibility of changing ref genomes
chromList = [
    "chr1",
    "chr2",
    "chr3",
    "chr4",
    "chr5",
    "chr6",
    "chr7",
    "chr8",
    "chr9",
    "chr10",
    "chr11",
    "chr12",
    "chr13",
    "chr14",
    "chr15",
    "chr16",
    "chr17",
    "chr18",
    "chr19",
    "chr20",
    "chr21",
    "chr22",
    "chrX",
    "chrY",
    "chrM",
]


def get_DBImport_path1(wildcards):
    """Define input files for rule HC_genotype_gvcfs."""
    return glob.glob(
        "results/HaplotypeCaller/DBImport/"
        + dt
        + "_"
        + wildcards.chrom
        + "/"
        + wildcards.chrom
        + "*/genomicsdb_meta_dir/genomicsdb_meta*.json"
    )


def get_DBImport_path2(wildcards):
    """Define input files for rule HC_genotype_gvcfs."""
    path = "".join(
        glob.glob("results/HaplotypeCaller/DBImport/" + dt + "_" + wildcards.chrom + "/*/__*/")
    )
    myList = []
    if os.path.exists(path):
        myList = [
            "AD.tdb",
            "AD_var.tdb",
            "ALT.tdb",
            "ALT_var.tdb",
            "BaseQRankSum.tdb",
            "__book_keeping.tdb.gz",
            "__coords.tdb",
            "DP_FORMAT.tdb",
            "DP.tdb",
            "DS.tdb",
            "END.tdb",
            "ExcessHet.tdb",
            "FILTER.tdb",
            "FILTER_var.tdb",
            "GQ.tdb",
            "GT.tdb",
            "GT_var.tdb",
            "ID.tdb",
            "ID_var.tdb",
            "InbreedingCoeff.tdb",
            "MIN_DP.tdb",
            "MLEAC.tdb",
            "MLEAC_var.tdb",
            "MLEAF.tdb",
            "MLEAF_var.tdb",
            "MQRankSum.tdb",
            "PGT.tdb",
            "PGT_var.tdb",
            "PID.tdb",
            "PID_var.tdb",
            "PL.tdb",
            "PL_var.tdb",
            "QUAL.tdb",
            "RAW_MQandDP.tdb",
            "ReadPosRankSum.tdb",
            "REF.tdb",
            "REF_var.tdb",
            "SB.tdb",
            "__tiledb_fragment.tdb",
        ]
        myList = [path + file for file in myList]
    return myList


include: "rules/HC_calling.smk"
include: "rules/filter.smk"


wildcard_constraints:
    sample="|".join(SAMPLES),
    rg="|".join(sampleDict.keys()),


rule all:
    input:
        "results/multiqc/multiqc.html",
        "results/HaplotypeCaller/filtered/HC.variant_calling_detail_metrics",


rule get_resources:
    """Programmatically retrieve Broad resources from an AWS s3 bucket.
    May want to allow a switch to their GCP bucket, depending on
    which cloud provider we're using.
    """
    output:
        "resources/Homo_sapiens_assembly38.fasta",
        "resources/Homo_sapiens_assembly38.dict",
        "resources/Homo_sapiens_assembly38.fasta.64.alt",
        "resources/Homo_sapiens_assembly38.fasta.64.amb",
        "resources/Homo_sapiens_assembly38.fasta.64.ann",
        "resources/Homo_sapiens_assembly38.fasta.64.bwt",
        "resources/Homo_sapiens_assembly38.fasta.64.pac",
        "resources/Homo_sapiens_assembly38.fasta.64.sa",
        "resources/Homo_sapiens_assembly38.fasta.fai",
        "resources/Homo_sapiens_assembly38.known_indels.vcf.gz",
        "resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi",
        "resources/Homo_sapiens_assembly38.dbsnp138.vcf",
        "resources/Homo_sapiens_assembly38.dbsnp138.vcf.idx",
    benchmark:
        "results/performance_benchmarks/get_resources/benchmarks.tsv"
    conda:
        "envs/aws.yaml"
    shell:
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dict resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.alt resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.amb resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.ann resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.bwt resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.pac resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.64.sa resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf resources/ --no-sign-request && "
        "aws s3 cp s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx resources/ --no-sign-request"


rule symlink_fastqs:
    """Create symbolic links with a different naming scheme.
    FastQC doesn't allow you to change the output file names, so
    this is a way to get the sample and readgroup info into the reports
    and use the filenaming convention followed in the rest of the pipeline.

    Note that we're assuming paired end data throughout.
    """
    input:
        r1=get_read1_fastq,
        r2=get_read2_fastq,
    output:
        r1="results/input/{sample}_{rg}_r1.fastq.gz",
        r2="results/input/{sample}_{rg}_r2.fastq.gz",
    benchmark:
        "results/performance_benchmarks/symlink_fastqs/{sample}_{rg}.tsv"
    shell:
        "ln -s {input.r1} {output.r1} && "
        "ln -s {input.r2} {output.r2}"


rule fastqc:
    """Generate FastQC reports for all input fastqs."""
    input:
        r1="results/input/{sample}_{rg}_r1.fastq.gz",
        r2="results/input/{sample}_{rg}_r2.fastq.gz",
    output:
        html1="results/fastqc/{sample}_{rg}_r1_fastqc.html",
        zip1="results/fastqc/{sample}_{rg}_r1_fastqc.zip",
        html2="results/fastqc/{sample}_{rg}_r2_fastqc.html",
        zip2="results/fastqc/{sample}_{rg}_r2_fastqc.zip",
    benchmark:
        "results/performance_benchmarks/fastqc/{sample}_{rg}.tsv"
    threads: nt
    conda:
        "envs/fastqc_multiqc.yaml"
    shell:
        "fastqc {input.r1} --quiet -t {threads} --outdir=results/fastqc/ && "
        "fastqc {input.r2} --quiet -t {threads} --outdir=results/fastqc/"


rule multiqc:
    """Generate one multiQC report for all input fastqs.
    Should add samtools stats output and possibly others eventually,
    dedup metrics, ...
    """
    input:
        expand("results/fastqc/{sample}_{rg}_r1_fastqc.zip", sample=SAMPLES, rg=sampleDict.keys()),
        expand("results/fastqc/{sample}_{rg}_r2_fastqc.zip", sample=SAMPLES, rg=sampleDict.keys()),
    output:
        "results/multiqc/multiqc.html",
    benchmark:
        "results/performance_benchmarks/multiqc/benchmarks.tsv"
    params:
        outDir="results/multiqc/",
        outName="multiqc.html",
    conda:
        "envs/fastqc_multiqc.yaml"
    shell:
        "multiqc --force -o {params.outDir} -n {params.outName} {input}"


rule quality_trimming:
    """Quality trimming of read ends.
    May want to tweak params; possibly put in config.  Could remove
    adapter sequences here if we want to move that downstream at
    some point.
    """
    input:
        r1=get_read1_fastq,
        r2=get_read2_fastq,
    output:
        r1_paired="results/paired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2_paired="results/paired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
        r1_unpaired="results/unpaired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2_unpaired="results/unpaired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
    benchmark:
        "results/performance_benchmarks/quality_trimming/{sample}_{rg}.tsv"
    params:
        lead=12,
        trail=12,
        window="4:15",
        minlen=36,
    threads: nt
    conda:
        "envs/trimmomatic.yaml"
    shell:
        "trimmomatic PE "
        "-threads {threads} "
        "-phred33 "
        "{input.r1} {input.r2} "
        "{output.r1_paired} {output.r1_unpaired} "
        "{output.r2_paired} {output.r2_unpaired} "
        "LEADING:{params.lead} "
        "TRAILING:{params.trail} "
        "SLIDINGWINDOW:{params.window} "
        "MINLEN:{params.minlen}" #ILLUMINACLIP:/path/to/adapters/TruSeq3-PE-2.fa:2:30:10


rule align_reads:
    """Align reads to reference with BWA.
    This step combines R1 and R2 data into a single output bam per
    fastq pair.  For samples where multiple fastq files represent a
    single sample, e.g. if a sample was run over several lanes, each
    R1/R1 fastq pair will be aligned separately here.  Then the aligned
    bams from a single sample will be combined during the dedup step
    below.  This should help modestly speed up alignment, as it's run
    in parallel for each fastq pair.
    """
    input:
        r="resources/Homo_sapiens_assembly38.fasta",
        r1="results/paired_trimmed_reads/{sample}/{rg}_r1.fastq.gz",
        r2="results/paired_trimmed_reads/{sample}/{rg}_r2.fastq.gz",
        d="resources/Homo_sapiens_assembly38.dict",
        fai="resources/Homo_sapiens_assembly38.fasta.fai",
        amb="resources/Homo_sapiens_assembly38.fasta.64.amb",
        ann="resources/Homo_sapiens_assembly38.fasta.64.ann",
        bwt="resources/Homo_sapiens_assembly38.fasta.64.bwt",
        pac="resources/Homo_sapiens_assembly38.fasta.64.pac",
        sa="resources/Homo_sapiens_assembly38.fasta.64.sa",
    output:
        temp("results/mapped/{sample}/{rg}.bam"),
    benchmark:
        "results/performance_benchmarks/align_reads/{sample}_{rg}.tsv"
    params:
        pl="ILLUMINA",
        sort_order="coordinate",
        rg="{rg}",
        sm=get_sm,
    threads: nt
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "bwa mem "
        "-K 10000000 -M "
        '-R "@RG\\tCN:54gene\\tID:{params.rg}\\tSM:{params.sm}\\tPL:{params.pl}\\tLB:N/A" '
        "-t {threads} "
        "{input.r} {input.r1} {input.r2} | "
        "samtools sort -@ {threads} -o {output} - " # note - wanted to use bwa-mem2, but doesn't seem to be working via conda


rule mark_duplicates:
    """Mark duplicates and merge multiple bams per sample.

    If there are multiple fastq pairs per sample (e.g. if the sample
    is split over multiple lanes), I am waiting to merge them until
    this step, instead of concatenating them prior to alignment.  This
    is to help maintain some additional parallelization going in to
    alignment.
    """
    input:
        expand("results/mapped/{{sample}}/{rg}.bam", rg=sampleDict.keys()),
    output:
        bam=temp("results/dedup/{sample}.bam"),
        metrics="results/dedup/{sample}.metrics.txt",
    benchmark:
        "results/performance_benchmarks/mark_duplicates/{sample}.tsv"
    params:
        list_markdup_inputs,
    conda:
        "envs/picard.yaml"
    shell:
        "picard MarkDuplicates "
        "REMOVE_DUPLICATES=true "
        "INPUT={params} "
        "OUTPUT={output.bam} "
        "METRICS_FILE={output.metrics}"


rule recalibrate_bams:
    """Calculate table for BQSR."""
    input:
        bam="results/dedup/{sample}.bam",
        r="resources/Homo_sapiens_assembly38.fasta",
        d="resources/Homo_sapiens_assembly38.dict",
        snps="resources/Homo_sapiens_assembly38.dbsnp138.vcf",
        s_index="resources/Homo_sapiens_assembly38.dbsnp138.vcf.idx",
        indels="resources/Homo_sapiens_assembly38.known_indels.vcf.gz",
        i_index="resources/Homo_sapiens_assembly38.known_indels.vcf.gz.tbi",
    output:
        "results/bqsr/{sample}.recal_table",
    benchmark:
        "results/performance_benchmarks/recalibrate_bams/{sample}.tsv"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk BaseRecalibrator "
        "-R {input.r} "
        "-I {input.bam} "
        "--known-sites {input.snps} "
        "--known-sites {input.indels} "
        "-O {output}"


rule apply_bqsr:
    """Recalibrate bams."""
    input:
        bam="results/dedup/{sample}.bam",
        r="resources/Homo_sapiens_assembly38.fasta",
        d="resources/Homo_sapiens_assembly38.dict",
        recal="results/bqsr/{sample}.recal_table",
    output:
        "results/bqsr/{sample}.bam",
    benchmark:
        "results/performance_benchmarks/apply_bqsr/{sample}.tsv"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk ApplyBQSR "
        "-R {input.r} "
        "-I {input.bam} "
        "--bqsr-recal-file {input.recal} "
        "-O {output}"


rule index_bams:
    """Index post-BQSR bams."""
    input:
        "results/bqsr/{sample}.bam",
    output:
        "results/bqsr/{sample}.bam.bai",
    benchmark:
        "results/performance_benchmarks/index_bams/{sample}.tsv"
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "samtools index {input} {output}"


rule samtools_stats:
    """Generate stats for bams.
    May want to do this pre-bqsr too.
    """
    input:
        "results/bqsr/{sample}.bam",
    output:
        "results/alignment_stats/{sample}.txt",
    benchmark:
        "results/performance_benchmarks/samtools_stats/{sample}.tsv"
    conda:
        "envs/bwa_samtools.yaml"
    shell:
        "samtools stats {input} > {output}"


rule split_bed_file:
    """Separates bed regions by chromosome.

    For DV:
    If you're not assigning a number of shards by which to divide
    and parallelize, then the pipeline will parallelize by chrom.
    To do this, we take the bed file (e.g. exome capture region)
    and split the regions by chromosome.  Subsequent steps are run
    concurrently on each of the single-chromosome bed files.

    For GATK:
    HaplotypeCaller can't be parallelized per task (e.g. threads),
    so must be run over sub-regions if you want parallelization.
    **Do we want to use the old 4000-region bed file, or is by-chrom
    sufficient?

    Note that grep exits with 0 if a match is found, 1 if no match,
    and 2 if error.  Snakemake looks for exit codes of 0 to determine
    that a job finished successfully.  No match is an acceptable outcome
    here, so the shell command below should allow match or no match.
    """
    input:
        bed,
    output:
        temp("results/split_regions/{chrom}.bed"),
    benchmark:
        "results/performance_benchmarks/split_bed_file/{chrom}.tsv"
    shell:
        'grep "^{wildcards.chrom}[[:space:]]" {input} > {output}; '
        "if [ $? -le 1 ]; then exit 0; else exit 1; fi"
